{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-26T19:40:58.314223Z",
     "start_time": "2024-05-26T19:40:56.125818Z"
    }
   },
   "source": [
    "import torch.cuda\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "SEED=1\n",
    "\n",
    "from src.henry.mnist_vae_pure import VariationalAutoencoder\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from utils import * \n",
    "from WeightsModification import * \n",
    "from federated_pure import federate\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:40:58.317851Z",
     "start_time": "2024-05-26T19:40:58.315295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FakeArgs:\n",
    "    def __init__(self):\n",
    "        self.seed=4237987\n",
    "        self.epochs = 10\n",
    "        self.dataset = \"mnist\"\n",
    "        self.num_users=4\n",
    "        self.iid = 3\n",
    "        self.dirichlet = 0.5 \n",
    "        self.local_ep=10\n",
    "        self.local_bs=64\n",
    "        self.beta=1.0\n",
    "        self.frac_split=0.5\n",
    "    # def __init__(self):\n",
    "    #     self.seed=SEED\n",
    "    #     self.epochs = 25\n",
    "    #     self.dataset = \"fmnist\"\n",
    "    #     self.num_users=10\n",
    "    #     self.iid = 2\n",
    "    #     self.dirichlet = 0.5 \n",
    "    #     self.local_ep=10\n",
    "    #     self.local_bs=64\n",
    "    #     self.beta=1.0\n",
    "        "
   ],
   "id": "eeb36afb17958ed6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:38:54.381015Z",
     "start_time": "2024-05-26T19:38:52.621896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alpha_values = [0.1,0.5,1.0,2.0]\n",
    "vae_beta_values = [1.0,5.0,10.0]\n",
    "\n",
    "for vae_beta_val in vae_beta_values:\n",
    "    runtime_args = FakeArgs()\n",
    "    runtime_args.beta=vae_beta_val\n",
    "    \n",
    "    baseline_result = federate(runtime_args)\n",
    "    baseline_result.serialise(\"baseline\", runtime_args)\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        new_weights = calculate_new_weights(baseline_result.global_model.encoder, baseline_result.client_datasets, alpha,0.0)\n",
    "\n",
    "        second_res = federate(runtime_args,new_weights, baseline_result.client_datasets)\n",
    "        second_res.serialise(\"alpha_\" + str(alpha),runtime_args)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1d8ac6a9e8515260",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ALL DONE!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m baseline_result\u001B[38;5;241m.\u001B[39mserialise(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbaseline\u001B[39m\u001B[38;5;124m\"\u001B[39m, runtime_args)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m alpha \u001B[38;5;129;01min\u001B[39;00m alpha_values:\n\u001B[0;32m---> 12\u001B[0m     new_weights \u001B[38;5;241m=\u001B[39m calculate_new_weights(baseline_result\u001B[38;5;241m.\u001B[39mglobal_model\u001B[38;5;241m.\u001B[39mencoder, baseline_result\u001B[38;5;241m.\u001B[39mclient_datasets, alpha,\u001B[38;5;241m0.0\u001B[39m)\n\u001B[1;32m     14\u001B[0m     second_res \u001B[38;5;241m=\u001B[39m federate(runtime_args,new_weights, baseline_result\u001B[38;5;241m.\u001B[39mclient_datasets)\n\u001B[1;32m     15\u001B[0m     second_res\u001B[38;5;241m.\u001B[39mserialise(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malpha_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(alpha),runtime_args)\n",
      "File \u001B[0;32m~/school/rp/weights/src/henry/../utils.py:258\u001B[0m, in \u001B[0;36mcalculate_new_weights\u001B[0;34m(encoder, client_dataset_wrappers, alpha, beta)\u001B[0m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculate_new_weights\u001B[39m(encoder, client_dataset_wrappers, alpha, beta):\n\u001B[0;32m--> 258\u001B[0m     distances \u001B[38;5;241m=\u001B[39m calculate_statistical_distance_across_all_clients(encoder, client_dataset_wrappers)\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# inverse_kl_distances = calculate_inverse_divergences(kl_distances)\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# return __calculate_new_weight(inverse_kl_distances, client_dataset_wrappers, gamma)\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m local_discrepancy_weights(distances, client_dataset_wrappers, alpha, beta)\n",
      "File \u001B[0;32m~/school/rp/weights/src/henry/../utils.py:205\u001B[0m, in \u001B[0;36mcalculate_statistical_distance_across_all_clients\u001B[0;34m(encoder, client_datasets)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_id, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(client_datasets):\n\u001B[1;32m    204\u001B[0m     data_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mtraining_subset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mtraining_subset))\n\u001B[0;32m--> 205\u001B[0m     x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(data_loader))\n\u001B[1;32m    206\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    207\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/utils/data/dataset.py:419\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    143\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001B[0m, in \u001B[0;36mNormalize.forward\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mnormalize(tensor, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstd, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minplace)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mnormalize(tensor, mean\u001B[38;5;241m=\u001B[39mmean, std\u001B[38;5;241m=\u001B[39mstd, inplace\u001B[38;5;241m=\u001B[39minplace)\n",
      "File \u001B[0;32m~/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:915\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtensor\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    912\u001B[0m     )\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inplace:\n\u001B[0;32m--> 915\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m    917\u001B[0m dtype \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    918\u001B[0m mean \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(mean, dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:57:05.538733Z",
     "start_time": "2024-05-26T19:41:00.302505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vae, ovr_losses, mse_losses, kl_losses = vae.train_model(dataset_train,16,10)\n",
    "# print(mse_losses)\n",
    "# print(kl_losses)\n",
    "# 46 -> 34\n",
    "res = federate(FakeArgs())\n",
    "res.serialise(\"BOGUS_TEST\",FakeArgs())\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a1afe9246a657416",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/anaconda3/envs/rp/lib/python3.12/site-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "Training user 0 in round 1\n",
      "Finished local epoch 1 out of 10, average loss across batches: 602.9863385139628\n",
      "Finished local epoch 2 out of 10, average loss across batches: 559.4985645050698\n",
      "Finished local epoch 3 out of 10, average loss across batches: 550.180893139129\n",
      "Finished local epoch 4 out of 10, average loss across batches: 545.2921407496675\n",
      "Finished local epoch 5 out of 10, average loss across batches: 541.8073885004571\n",
      "Finished local epoch 6 out of 10, average loss across batches: 539.5122345620014\n",
      "Finished local epoch 7 out of 10, average loss across batches: 537.8460931006898\n",
      "Finished local epoch 8 out of 10, average loss across batches: 536.3782021380486\n",
      "Finished local epoch 9 out of 10, average loss across batches: 535.0337334815492\n",
      "Finished local epoch 10 out of 10, average loss across batches: 533.3948304521276\n",
      "Training user 1 in round 1\n",
      "Finished local epoch 1 out of 10, average loss across batches: 594.215989392869\n",
      "Finished local epoch 2 out of 10, average loss across batches: 554.1404096887467\n",
      "Finished local epoch 3 out of 10, average loss across batches: 546.1993671823055\n",
      "Finished local epoch 4 out of 10, average loss across batches: 541.5565627077792\n",
      "Finished local epoch 5 out of 10, average loss across batches: 538.0745474308095\n",
      "Finished local epoch 6 out of 10, average loss across batches: 535.0299148884226\n",
      "Finished local epoch 7 out of 10, average loss across batches: 532.7707688351895\n",
      "Finished local epoch 8 out of 10, average loss across batches: 530.9480968718833\n",
      "Finished local epoch 9 out of 10, average loss across batches: 529.8874805206948\n",
      "Finished local epoch 10 out of 10, average loss across batches: 528.4546693193151\n",
      "Training user 2 in round 1\n",
      "Finished local epoch 1 out of 10, average loss across batches: 597.8626910593715\n",
      "Finished local epoch 2 out of 10, average loss across batches: 547.2127511555253\n",
      "Finished local epoch 3 out of 10, average loss across batches: 541.7911956925198\n",
      "Finished local epoch 4 out of 10, average loss across batches: 538.6812212499558\n",
      "Finished local epoch 5 out of 10, average loss across batches: 535.601281765899\n",
      "Finished local epoch 6 out of 10, average loss across batches: 533.4030122368583\n",
      "Finished local epoch 7 out of 10, average loss across batches: 531.0738361065204\n",
      "Finished local epoch 8 out of 10, average loss across batches: 528.7173965937411\n",
      "Finished local epoch 9 out of 10, average loss across batches: 526.567317358509\n",
      "Finished local epoch 10 out of 10, average loss across batches: 525.3507193310768\n",
      "Training user 3 in round 1\n",
      "Finished local epoch 1 out of 10, average loss across batches: 575.8412883712584\n",
      "Finished local epoch 2 out of 10, average loss across batches: 535.4322979172549\n",
      "Finished local epoch 3 out of 10, average loss across batches: 527.9010599281893\n",
      "Finished local epoch 4 out of 10, average loss across batches: 523.8715741873744\n",
      "Finished local epoch 5 out of 10, average loss across batches: 520.918569051597\n",
      "Finished local epoch 6 out of 10, average loss across batches: 518.7244044537525\n",
      "Finished local epoch 7 out of 10, average loss across batches: 516.7519164793941\n",
      "Finished local epoch 8 out of 10, average loss across batches: 515.8971838280858\n",
      "Finished local epoch 9 out of 10, average loss across batches: 514.1335260475496\n",
      "Finished local epoch 10 out of 10, average loss across batches: 513.3031476491905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:36<14:31, 96.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 1 totalL: 644.2698196777344\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "Training user 0 in round 2\n",
      "Finished local epoch 1 out of 10, average loss across batches: 546.0226987148853\n",
      "Finished local epoch 2 out of 10, average loss across batches: 530.7368047186669\n",
      "Finished local epoch 3 out of 10, average loss across batches: 526.9870576899102\n",
      "Finished local epoch 4 out of 10, average loss across batches: 524.4914719601895\n",
      "Finished local epoch 5 out of 10, average loss across batches: 522.4995634038397\n",
      "Finished local epoch 6 out of 10, average loss across batches: 520.5854203893783\n",
      "Finished local epoch 7 out of 10, average loss across batches: 519.823358414021\n",
      "Finished local epoch 8 out of 10, average loss across batches: 518.1956171563331\n",
      "Finished local epoch 9 out of 10, average loss across batches: 517.4541340280086\n",
      "Finished local epoch 10 out of 10, average loss across batches: 516.787128204995\n",
      "Training user 1 in round 2\n",
      "Finished local epoch 1 out of 10, average loss across batches: 547.3803556401679\n",
      "Finished local epoch 2 out of 10, average loss across batches: 531.5206231299867\n",
      "Finished local epoch 3 out of 10, average loss across batches: 527.2565782912234\n",
      "Finished local epoch 4 out of 10, average loss across batches: 524.4132112543633\n",
      "Finished local epoch 5 out of 10, average loss across batches: 522.3431995148354\n",
      "Finished local epoch 6 out of 10, average loss across batches: 520.9777952802942\n",
      "Finished local epoch 7 out of 10, average loss across batches: 519.440640063996\n",
      "Finished local epoch 8 out of 10, average loss across batches: 518.1105444076213\n",
      "Finished local epoch 9 out of 10, average loss across batches: 516.8891906738281\n",
      "Finished local epoch 10 out of 10, average loss across batches: 516.1203244473072\n",
      "Training user 2 in round 2\n",
      "Finished local epoch 1 out of 10, average loss across batches: 547.3938055944659\n",
      "Finished local epoch 2 out of 10, average loss across batches: 529.8153421393347\n",
      "Finished local epoch 3 out of 10, average loss across batches: 525.6500083957862\n",
      "Finished local epoch 4 out of 10, average loss across batches: 523.1641159402839\n",
      "Finished local epoch 5 out of 10, average loss across batches: 520.9031274027415\n",
      "Finished local epoch 6 out of 10, average loss across batches: 519.5832373157346\n",
      "Finished local epoch 7 out of 10, average loss across batches: 518.2906320149003\n",
      "Finished local epoch 8 out of 10, average loss across batches: 517.4549581260163\n",
      "Finished local epoch 9 out of 10, average loss across batches: 516.3694354441371\n",
      "Finished local epoch 10 out of 10, average loss across batches: 515.6259130417492\n",
      "Training user 3 in round 2\n",
      "Finished local epoch 1 out of 10, average loss across batches: 526.6701601327184\n",
      "Finished local epoch 2 out of 10, average loss across batches: 511.935562562751\n",
      "Finished local epoch 3 out of 10, average loss across batches: 509.0184494079835\n",
      "Finished local epoch 4 out of 10, average loss across batches: 507.11097858229795\n",
      "Finished local epoch 5 out of 10, average loss across batches: 505.3808147629581\n",
      "Finished local epoch 6 out of 10, average loss across batches: 504.00946253274816\n",
      "Finished local epoch 7 out of 10, average loss across batches: 502.96685214981017\n",
      "Finished local epoch 8 out of 10, average loss across batches: 501.8208913534999\n",
      "Finished local epoch 9 out of 10, average loss across batches: 501.22630218138175\n",
      "Finished local epoch 10 out of 10, average loss across batches: 500.26227970965897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [03:08<12:29, 93.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 2 totalL: 548.6690906524658\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "Training user 0 in round 3\n",
      "Finished local epoch 1 out of 10, average loss across batches: 521.239535717254\n",
      "Finished local epoch 2 out of 10, average loss across batches: 516.0522988177361\n",
      "Finished local epoch 3 out of 10, average loss across batches: 513.7901595744681\n",
      "Finished local epoch 4 out of 10, average loss across batches: 512.4190043997257\n",
      "Finished local epoch 5 out of 10, average loss across batches: 511.0862790371509\n",
      "Finished local epoch 6 out of 10, average loss across batches: 510.2740616169382\n",
      "Finished local epoch 7 out of 10, average loss across batches: 509.67458548038564\n",
      "Finished local epoch 8 out of 10, average loss across batches: 509.0019372818318\n",
      "Finished local epoch 9 out of 10, average loss across batches: 507.6564067434757\n",
      "Finished local epoch 10 out of 10, average loss across batches: 507.4381546345163\n",
      "Training user 1 in round 3\n",
      "Finished local epoch 1 out of 10, average loss across batches: 522.3387810889711\n",
      "Finished local epoch 2 out of 10, average loss across batches: 516.4051451338098\n",
      "Finished local epoch 3 out of 10, average loss across batches: 514.554082732505\n",
      "Finished local epoch 4 out of 10, average loss across batches: 513.1463095807014\n",
      "Finished local epoch 5 out of 10, average loss across batches: 512.1006777499584\n",
      "Finished local epoch 6 out of 10, average loss across batches: 511.3016998940326\n",
      "Finished local epoch 7 out of 10, average loss across batches: 510.6487130672374\n",
      "Finished local epoch 8 out of 10, average loss across batches: 510.0024694564495\n",
      "Finished local epoch 9 out of 10, average loss across batches: 509.2293723248421\n",
      "Finished local epoch 10 out of 10, average loss across batches: 508.4556028974817\n",
      "Training user 2 in round 3\n",
      "Finished local epoch 1 out of 10, average loss across batches: 527.7343984750601\n",
      "Finished local epoch 2 out of 10, average loss across batches: 519.0689342377951\n",
      "Finished local epoch 3 out of 10, average loss across batches: 516.3875587428856\n",
      "Finished local epoch 4 out of 10, average loss across batches: 514.3253770370828\n",
      "Finished local epoch 5 out of 10, average loss across batches: 513.0081649020786\n",
      "Finished local epoch 6 out of 10, average loss across batches: 511.91451653528\n",
      "Finished local epoch 7 out of 10, average loss across batches: 510.6969447071196\n",
      "Finished local epoch 8 out of 10, average loss across batches: 509.43300576878886\n",
      "Finished local epoch 9 out of 10, average loss across batches: 509.1097859516403\n",
      "Finished local epoch 10 out of 10, average loss across batches: 508.3405845952789\n",
      "Training user 3 in round 3\n",
      "Finished local epoch 1 out of 10, average loss across batches: 509.2724827532787\n",
      "Finished local epoch 2 out of 10, average loss across batches: 503.59380257847795\n",
      "Finished local epoch 3 out of 10, average loss across batches: 501.56593647539376\n",
      "Finished local epoch 4 out of 10, average loss across batches: 500.6283786268119\n",
      "Finished local epoch 5 out of 10, average loss across batches: 500.1788473473974\n",
      "Finished local epoch 6 out of 10, average loss across batches: 498.44575445336034\n",
      "Finished local epoch 7 out of 10, average loss across batches: 498.48027963906407\n",
      "Finished local epoch 8 out of 10, average loss across batches: 497.272427355908\n",
      "Finished local epoch 9 out of 10, average loss across batches: 496.1987683399614\n",
      "Finished local epoch 10 out of 10, average loss across batches: 495.7092103766629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:39<10:48, 92.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 3 totalL: 520.7849924484253\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "Training user 0 in round 4\n",
      "Finished local epoch 1 out of 10, average loss across batches: 510.8073740857713\n",
      "Finished local epoch 2 out of 10, average loss across batches: 507.7769539041722\n",
      "Finished local epoch 3 out of 10, average loss across batches: 506.38096508269615\n",
      "Finished local epoch 4 out of 10, average loss across batches: 505.9467203343168\n",
      "Finished local epoch 5 out of 10, average loss across batches: 505.7051692881483\n",
      "Finished local epoch 6 out of 10, average loss across batches: 504.9602659834192\n",
      "Finished local epoch 7 out of 10, average loss across batches: 504.24322795462103\n",
      "Finished local epoch 8 out of 10, average loss across batches: 504.079417127244\n",
      "Finished local epoch 9 out of 10, average loss across batches: 503.52627485559344\n",
      "Finished local epoch 10 out of 10, average loss across batches: 503.23359167220747\n",
      "Training user 1 in round 4\n",
      "Finished local epoch 1 out of 10, average loss across batches: 511.48457524725734\n",
      "Finished local epoch 2 out of 10, average loss across batches: 508.73684510575964\n",
      "Finished local epoch 3 out of 10, average loss across batches: 508.1480135004571\n",
      "Finished local epoch 4 out of 10, average loss across batches: 506.90105629778924\n",
      "Finished local epoch 5 out of 10, average loss across batches: 505.9767006732048\n",
      "Finished local epoch 6 out of 10, average loss across batches: 505.6095156405834\n",
      "Finished local epoch 7 out of 10, average loss across batches: 505.26535112096906\n",
      "Finished local epoch 8 out of 10, average loss across batches: 505.1479168831034\n",
      "Finished local epoch 9 out of 10, average loss across batches: 504.267461768617\n",
      "Finished local epoch 10 out of 10, average loss across batches: 504.3783348570479\n",
      "Training user 2 in round 4\n",
      "Finished local epoch 1 out of 10, average loss across batches: 516.5486933505373\n",
      "Finished local epoch 2 out of 10, average loss across batches: 511.6332365700562\n",
      "Finished local epoch 3 out of 10, average loss across batches: 510.19257613652434\n",
      "Finished local epoch 4 out of 10, average loss across batches: 509.1443775574007\n",
      "Finished local epoch 5 out of 10, average loss across batches: 508.08175231105065\n",
      "Finished local epoch 6 out of 10, average loss across batches: 506.6047668457031\n",
      "Finished local epoch 7 out of 10, average loss across batches: 506.08065312588377\n",
      "Finished local epoch 8 out of 10, average loss across batches: 505.25982721251063\n",
      "Finished local epoch 9 out of 10, average loss across batches: 504.6337864388168\n",
      "Finished local epoch 10 out of 10, average loss across batches: 504.2477784307834\n",
      "Training user 3 in round 4\n",
      "Finished local epoch 1 out of 10, average loss across batches: 501.91100007941924\n",
      "Finished local epoch 2 out of 10, average loss across batches: 498.63371993834716\n",
      "Finished local epoch 3 out of 10, average loss across batches: 497.8114903461502\n",
      "Finished local epoch 4 out of 10, average loss across batches: 496.880008314508\n",
      "Finished local epoch 5 out of 10, average loss across batches: 496.3597989369588\n",
      "Finished local epoch 6 out of 10, average loss across batches: 495.50569440849335\n",
      "Finished local epoch 7 out of 10, average loss across batches: 495.3116937966711\n",
      "Finished local epoch 8 out of 10, average loss across batches: 494.74856175189035\n",
      "Finished local epoch 9 out of 10, average loss across batches: 493.9911571334165\n",
      "Finished local epoch 10 out of 10, average loss across batches: 493.71683425596916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [06:12<09:16, 92.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 4 totalL: 512.3407841445922\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "Training user 0 in round 5\n",
      "Finished local epoch 1 out of 10, average loss across batches: 505.79486006067157\n",
      "Finished local epoch 2 out of 10, average loss across batches: 504.31311242935504\n",
      "Finished local epoch 3 out of 10, average loss across batches: 503.1950582301363\n",
      "Finished local epoch 4 out of 10, average loss across batches: 502.605893268991\n",
      "Finished local epoch 5 out of 10, average loss across batches: 502.1882125529837\n",
      "Finished local epoch 6 out of 10, average loss across batches: 501.5229966183926\n",
      "Finished local epoch 7 out of 10, average loss across batches: 501.15953862616357\n",
      "Finished local epoch 8 out of 10, average loss across batches: 501.0807567839927\n",
      "Finished local epoch 9 out of 10, average loss across batches: 500.43857512778425\n",
      "Finished local epoch 10 out of 10, average loss across batches: 500.04865398001164\n",
      "Training user 1 in round 5\n",
      "Finished local epoch 1 out of 10, average loss across batches: 506.5521279192985\n",
      "Finished local epoch 2 out of 10, average loss across batches: 504.7334431100399\n",
      "Finished local epoch 3 out of 10, average loss across batches: 503.7343849993767\n",
      "Finished local epoch 4 out of 10, average loss across batches: 503.27547529504653\n",
      "Finished local epoch 5 out of 10, average loss across batches: 503.09109315263464\n",
      "Finished local epoch 6 out of 10, average loss across batches: 502.3323846045961\n",
      "Finished local epoch 7 out of 10, average loss across batches: 502.02808071704624\n",
      "Finished local epoch 8 out of 10, average loss across batches: 501.57156566863364\n",
      "Finished local epoch 9 out of 10, average loss across batches: 501.3397018107962\n",
      "Finished local epoch 10 out of 10, average loss across batches: 501.00192247755984\n",
      "Training user 2 in round 5\n",
      "Finished local epoch 1 out of 10, average loss across batches: 511.4712527175834\n",
      "Finished local epoch 2 out of 10, average loss across batches: 508.6454663859234\n",
      "Finished local epoch 3 out of 10, average loss across batches: 506.39747529871323\n",
      "Finished local epoch 4 out of 10, average loss across batches: 506.58290272708393\n",
      "Finished local epoch 5 out of 10, average loss across batches: 505.16737648804263\n",
      "Finished local epoch 6 out of 10, average loss across batches: 504.49978030105524\n",
      "Finished local epoch 7 out of 10, average loss across batches: 503.7432556152344\n",
      "Finished local epoch 8 out of 10, average loss across batches: 503.1195631760817\n",
      "Finished local epoch 9 out of 10, average loss across batches: 502.92681290984694\n",
      "Finished local epoch 10 out of 10, average loss across batches: 502.5213337203496\n",
      "Training user 3 in round 5\n",
      "Finished local epoch 1 out of 10, average loss across batches: 499.301117311041\n",
      "Finished local epoch 2 out of 10, average loss across batches: 497.38173188358905\n",
      "Finished local epoch 3 out of 10, average loss across batches: 496.21352634659735\n",
      "Finished local epoch 4 out of 10, average loss across batches: 494.92243436637176\n",
      "Finished local epoch 5 out of 10, average loss across batches: 494.4077187656878\n",
      "Finished local epoch 6 out of 10, average loss across batches: 494.0688095399175\n",
      "Finished local epoch 7 out of 10, average loss across batches: 493.16295271417704\n",
      "Finished local epoch 8 out of 10, average loss across batches: 492.7649162323121\n",
      "Finished local epoch 9 out of 10, average loss across batches: 492.67325331599835\n",
      "Finished local epoch 10 out of 10, average loss across batches: 491.95120325050203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:52<07:56, 95.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 5 totalL: 508.40311206970216\n",
      "\n",
      " | Global Training Round : 6 |\n",
      "\n",
      "Training user 0 in round 6\n",
      "Finished local epoch 1 out of 10, average loss across batches: 502.7263379685422\n",
      "Finished local epoch 2 out of 10, average loss across batches: 501.22321115047373\n",
      "Finished local epoch 3 out of 10, average loss across batches: 500.63120389897773\n",
      "Finished local epoch 4 out of 10, average loss across batches: 500.41452623732545\n",
      "Finished local epoch 5 out of 10, average loss across batches: 500.62145255880154\n",
      "Finished local epoch 6 out of 10, average loss across batches: 499.52223887342086\n",
      "Finished local epoch 7 out of 10, average loss across batches: 499.04902213887965\n",
      "Finished local epoch 8 out of 10, average loss across batches: 498.7768036537982\n",
      "Finished local epoch 9 out of 10, average loss across batches: 498.4002298558012\n",
      "Finished local epoch 10 out of 10, average loss across batches: 498.086023858253\n",
      "Training user 1 in round 6\n",
      "Finished local epoch 1 out of 10, average loss across batches: 503.62623888380983\n",
      "Finished local epoch 2 out of 10, average loss across batches: 502.14534652385305\n",
      "Finished local epoch 3 out of 10, average loss across batches: 501.75517850835274\n",
      "Finished local epoch 4 out of 10, average loss across batches: 501.1407436938996\n",
      "Finished local epoch 5 out of 10, average loss across batches: 500.4814290797457\n",
      "Finished local epoch 6 out of 10, average loss across batches: 500.29442177630483\n",
      "Finished local epoch 7 out of 10, average loss across batches: 500.09404881254153\n",
      "Finished local epoch 8 out of 10, average loss across batches: 499.09487395590924\n",
      "Finished local epoch 9 out of 10, average loss across batches: 498.87180305643284\n",
      "Finished local epoch 10 out of 10, average loss across batches: 498.7919128417969\n",
      "Training user 2 in round 6\n",
      "Finished local epoch 1 out of 10, average loss across batches: 508.42119284237134\n",
      "Finished local epoch 2 out of 10, average loss across batches: 506.4092443129596\n",
      "Finished local epoch 3 out of 10, average loss across batches: 505.11456782138185\n",
      "Finished local epoch 4 out of 10, average loss across batches: 504.21265642675337\n",
      "Finished local epoch 5 out of 10, average loss across batches: 503.09195747202875\n",
      "Finished local epoch 6 out of 10, average loss across batches: 502.56002089556523\n",
      "Finished local epoch 7 out of 10, average loss across batches: 502.4251216008113\n",
      "Finished local epoch 8 out of 10, average loss across batches: 501.50902270839225\n",
      "Finished local epoch 9 out of 10, average loss across batches: 500.98401652202347\n",
      "Finished local epoch 10 out of 10, average loss across batches: 500.85490824841804\n",
      "Training user 3 in round 6\n",
      "Finished local epoch 1 out of 10, average loss across batches: 496.2274142958553\n",
      "Finished local epoch 2 out of 10, average loss across batches: 494.53849345134444\n",
      "Finished local epoch 3 out of 10, average loss across batches: 494.00680872905684\n",
      "Finished local epoch 4 out of 10, average loss across batches: 493.40817775496515\n",
      "Finished local epoch 5 out of 10, average loss across batches: 492.76835234002414\n",
      "Finished local epoch 6 out of 10, average loss across batches: 492.41912265762267\n",
      "Finished local epoch 7 out of 10, average loss across batches: 492.0333668659011\n",
      "Finished local epoch 8 out of 10, average loss across batches: 491.97241088376944\n",
      "Finished local epoch 9 out of 10, average loss across batches: 492.0847204736916\n",
      "Finished local epoch 10 out of 10, average loss across batches: 491.4258538058484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [09:33<06:29, 97.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 6 totalL: 505.54080164642335\n",
      "\n",
      " | Global Training Round : 7 |\n",
      "\n",
      "Training user 0 in round 7\n",
      "Finished local epoch 1 out of 10, average loss across batches: 500.6876262258976\n",
      "Finished local epoch 2 out of 10, average loss across batches: 499.6476176809757\n",
      "Finished local epoch 3 out of 10, average loss across batches: 498.7477280637051\n",
      "Finished local epoch 4 out of 10, average loss across batches: 498.1512661548371\n",
      "Finished local epoch 5 out of 10, average loss across batches: 498.0579297654172\n",
      "Finished local epoch 6 out of 10, average loss across batches: 497.7988760440908\n",
      "Finished local epoch 7 out of 10, average loss across batches: 497.59491395341587\n",
      "Finished local epoch 8 out of 10, average loss across batches: 497.3630111369681\n",
      "Finished local epoch 9 out of 10, average loss across batches: 496.675086358253\n",
      "Finished local epoch 10 out of 10, average loss across batches: 496.2076595225233\n",
      "Training user 1 in round 7\n",
      "Finished local epoch 1 out of 10, average loss across batches: 501.20088423059343\n",
      "Finished local epoch 2 out of 10, average loss across batches: 500.2319099588597\n",
      "Finished local epoch 3 out of 10, average loss across batches: 499.2752006368434\n",
      "Finished local epoch 4 out of 10, average loss across batches: 498.8174643658577\n",
      "Finished local epoch 5 out of 10, average loss across batches: 498.9588580192404\n",
      "Finished local epoch 6 out of 10, average loss across batches: 498.64079291161073\n",
      "Finished local epoch 7 out of 10, average loss across batches: 498.4457098778258\n",
      "Finished local epoch 8 out of 10, average loss across batches: 497.906265063996\n",
      "Finished local epoch 9 out of 10, average loss across batches: 497.5665596170628\n",
      "Finished local epoch 10 out of 10, average loss across batches: 497.3601106684259\n",
      "Training user 2 in round 7\n",
      "Finished local epoch 1 out of 10, average loss across batches: 505.74326431481546\n",
      "Finished local epoch 2 out of 10, average loss across batches: 503.3015415657699\n",
      "Finished local epoch 3 out of 10, average loss across batches: 503.15652396776017\n",
      "Finished local epoch 4 out of 10, average loss across batches: 502.1531653771034\n",
      "Finished local epoch 5 out of 10, average loss across batches: 501.6337125614218\n",
      "Finished local epoch 6 out of 10, average loss across batches: 501.30300143833074\n",
      "Finished local epoch 7 out of 10, average loss across batches: 500.4667241023137\n",
      "Finished local epoch 8 out of 10, average loss across batches: 499.49405542434073\n",
      "Finished local epoch 9 out of 10, average loss across batches: 499.39612810644087\n",
      "Finished local epoch 10 out of 10, average loss across batches: 499.43773350780367\n",
      "Training user 3 in round 7\n",
      "Finished local epoch 1 out of 10, average loss across batches: 494.4096504425907\n",
      "Finished local epoch 2 out of 10, average loss across batches: 493.51351107555223\n",
      "Finished local epoch 3 out of 10, average loss across batches: 492.9025424206592\n",
      "Finished local epoch 4 out of 10, average loss across batches: 492.18492255153427\n",
      "Finished local epoch 5 out of 10, average loss across batches: 491.7869004092542\n",
      "Finished local epoch 6 out of 10, average loss across batches: 491.09772917353007\n",
      "Finished local epoch 7 out of 10, average loss across batches: 490.7552574801158\n",
      "Finished local epoch 8 out of 10, average loss across batches: 490.2328958166651\n",
      "Finished local epoch 9 out of 10, average loss across batches: 489.7218044541447\n",
      "Finished local epoch 10 out of 10, average loss across batches: 490.2447669094346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [11:16<04:57, 99.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 7 totalL: 504.6509317840576\n",
      "\n",
      " | Global Training Round : 8 |\n",
      "\n",
      "Training user 0 in round 8\n",
      "Finished local epoch 1 out of 10, average loss across batches: 499.3828186035156\n",
      "Finished local epoch 2 out of 10, average loss across batches: 497.87893533909573\n",
      "Finished local epoch 3 out of 10, average loss across batches: 497.4903103442902\n",
      "Finished local epoch 4 out of 10, average loss across batches: 496.880296033494\n",
      "Finished local epoch 5 out of 10, average loss across batches: 496.6341955306682\n",
      "Finished local epoch 6 out of 10, average loss across batches: 496.1322800656582\n",
      "Finished local epoch 7 out of 10, average loss across batches: 496.89332496156084\n",
      "Finished local epoch 8 out of 10, average loss across batches: 496.20621779421543\n",
      "Finished local epoch 9 out of 10, average loss across batches: 495.57628316676363\n",
      "Finished local epoch 10 out of 10, average loss across batches: 495.76286335397276\n",
      "Training user 1 in round 8\n",
      "Finished local epoch 1 out of 10, average loss across batches: 500.0268985829455\n",
      "Finished local epoch 2 out of 10, average loss across batches: 499.83234915226063\n",
      "Finished local epoch 3 out of 10, average loss across batches: 499.13261991460274\n",
      "Finished local epoch 4 out of 10, average loss across batches: 498.3579052214927\n",
      "Finished local epoch 5 out of 10, average loss across batches: 497.7795343926612\n",
      "Finished local epoch 6 out of 10, average loss across batches: 497.37545529629324\n",
      "Finished local epoch 7 out of 10, average loss across batches: 496.9806878272523\n",
      "Finished local epoch 8 out of 10, average loss across batches: 496.90511033078457\n",
      "Finished local epoch 9 out of 10, average loss across batches: 496.41952605551865\n",
      "Finished local epoch 10 out of 10, average loss across batches: 496.1635261697972\n",
      "Training user 2 in round 8\n",
      "Finished local epoch 1 out of 10, average loss across batches: 504.7199469518877\n",
      "Finished local epoch 2 out of 10, average loss across batches: 502.4012471885163\n",
      "Finished local epoch 3 out of 10, average loss across batches: 501.85071242664736\n",
      "Finished local epoch 4 out of 10, average loss across batches: 501.14120276265555\n",
      "Finished local epoch 5 out of 10, average loss across batches: 501.1306301479426\n",
      "Finished local epoch 6 out of 10, average loss across batches: 500.4759753473204\n",
      "Finished local epoch 7 out of 10, average loss across batches: 499.61951346418977\n",
      "Finished local epoch 8 out of 10, average loss across batches: 500.01749844788424\n",
      "Finished local epoch 9 out of 10, average loss across batches: 499.7928059435538\n",
      "Finished local epoch 10 out of 10, average loss across batches: 499.318850003756\n",
      "Training user 3 in round 8\n",
      "Finished local epoch 1 out of 10, average loss across batches: 493.76073630459337\n",
      "Finished local epoch 2 out of 10, average loss across batches: 492.08917604009787\n",
      "Finished local epoch 3 out of 10, average loss across batches: 492.15741236430097\n",
      "Finished local epoch 4 out of 10, average loss across batches: 491.7999228358747\n",
      "Finished local epoch 5 out of 10, average loss across batches: 491.1171378629753\n",
      "Finished local epoch 6 out of 10, average loss across batches: 490.42366940908164\n",
      "Finished local epoch 7 out of 10, average loss across batches: 490.4240226286003\n",
      "Finished local epoch 8 out of 10, average loss across batches: 489.2986549469362\n",
      "Finished local epoch 9 out of 10, average loss across batches: 489.3975034660125\n",
      "Finished local epoch 10 out of 10, average loss across batches: 489.1519734945642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [12:56<03:18, 99.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 8 totalL: 502.74280419006345\n",
      "\n",
      " | Global Training Round : 9 |\n",
      "\n",
      "Training user 0 in round 9\n",
      "Finished local epoch 1 out of 10, average loss across batches: 497.42659236826796\n",
      "Finished local epoch 2 out of 10, average loss across batches: 496.6466879986702\n",
      "Finished local epoch 3 out of 10, average loss across batches: 496.23497353411733\n",
      "Finished local epoch 4 out of 10, average loss across batches: 496.6090534616024\n",
      "Finished local epoch 5 out of 10, average loss across batches: 495.932173968376\n",
      "Finished local epoch 6 out of 10, average loss across batches: 495.47411083464925\n",
      "Finished local epoch 7 out of 10, average loss across batches: 495.3647024601064\n",
      "Finished local epoch 8 out of 10, average loss across batches: 494.5083406488946\n",
      "Finished local epoch 9 out of 10, average loss across batches: 494.295030829247\n",
      "Finished local epoch 10 out of 10, average loss across batches: 493.9797368475731\n",
      "Training user 1 in round 9\n",
      "Finished local epoch 1 out of 10, average loss across batches: 498.340869270487\n",
      "Finished local epoch 2 out of 10, average loss across batches: 497.5178805736785\n",
      "Finished local epoch 3 out of 10, average loss across batches: 497.6217216329372\n",
      "Finished local epoch 4 out of 10, average loss across batches: 496.75887710895944\n",
      "Finished local epoch 5 out of 10, average loss across batches: 496.5791571434508\n",
      "Finished local epoch 6 out of 10, average loss across batches: 496.857425770861\n",
      "Finished local epoch 7 out of 10, average loss across batches: 496.11830781977227\n",
      "Finished local epoch 8 out of 10, average loss across batches: 495.5702617239445\n",
      "Finished local epoch 9 out of 10, average loss across batches: 495.70892333984375\n",
      "Finished local epoch 10 out of 10, average loss across batches: 495.81145383144946\n",
      "Training user 2 in round 9\n",
      "Finished local epoch 1 out of 10, average loss across batches: 503.0899259127103\n",
      "Finished local epoch 2 out of 10, average loss across batches: 501.708733191857\n",
      "Finished local epoch 3 out of 10, average loss across batches: 500.7135316322292\n",
      "Finished local epoch 4 out of 10, average loss across batches: 500.28912022103015\n",
      "Finished local epoch 5 out of 10, average loss across batches: 499.6097634432003\n",
      "Finished local epoch 6 out of 10, average loss across batches: 499.10182362552143\n",
      "Finished local epoch 7 out of 10, average loss across batches: 499.6047755452842\n",
      "Finished local epoch 8 out of 10, average loss across batches: 498.68378168856935\n",
      "Finished local epoch 9 out of 10, average loss across batches: 498.5073710307816\n",
      "Finished local epoch 10 out of 10, average loss across batches: 498.24446913775273\n",
      "Training user 3 in round 9\n",
      "Finished local epoch 1 out of 10, average loss across batches: 492.4719734651497\n",
      "Finished local epoch 2 out of 10, average loss across batches: 491.01454328054405\n",
      "Finished local epoch 3 out of 10, average loss across batches: 489.9949090796781\n",
      "Finished local epoch 4 out of 10, average loss across batches: 489.8033769599884\n",
      "Finished local epoch 5 out of 10, average loss across batches: 489.5249088394594\n",
      "Finished local epoch 6 out of 10, average loss across batches: 489.24625687809834\n",
      "Finished local epoch 7 out of 10, average loss across batches: 489.0639115299087\n",
      "Finished local epoch 8 out of 10, average loss across batches: 488.82170920008156\n",
      "Finished local epoch 9 out of 10, average loss across batches: 488.9655249415631\n",
      "Finished local epoch 10 out of 10, average loss across batches: 488.9856865204961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [14:33<01:38, 98.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 9 totalL: 501.75038607177737\n",
      "\n",
      " | Global Training Round : 10 |\n",
      "\n",
      "Training user 0 in round 10\n",
      "Finished local epoch 1 out of 10, average loss across batches: 496.3808005474983\n",
      "Finished local epoch 2 out of 10, average loss across batches: 495.66509581220913\n",
      "Finished local epoch 3 out of 10, average loss across batches: 494.9461414093667\n",
      "Finished local epoch 4 out of 10, average loss across batches: 494.4186406561669\n",
      "Finished local epoch 5 out of 10, average loss across batches: 494.99949834296046\n",
      "Finished local epoch 6 out of 10, average loss across batches: 494.6936002690741\n",
      "Finished local epoch 7 out of 10, average loss across batches: 494.1240477217005\n",
      "Finished local epoch 8 out of 10, average loss across batches: 493.70659400452956\n",
      "Finished local epoch 9 out of 10, average loss across batches: 493.64997961166057\n",
      "Finished local epoch 10 out of 10, average loss across batches: 493.2819191790642\n",
      "Training user 1 in round 10\n",
      "Finished local epoch 1 out of 10, average loss across batches: 497.81601133955286\n",
      "Finished local epoch 2 out of 10, average loss across batches: 496.71059894967584\n",
      "Finished local epoch 3 out of 10, average loss across batches: 496.7158000540226\n",
      "Finished local epoch 4 out of 10, average loss across batches: 495.5819472292636\n",
      "Finished local epoch 5 out of 10, average loss across batches: 494.924030839636\n",
      "Finished local epoch 6 out of 10, average loss across batches: 495.2981793342753\n",
      "Finished local epoch 7 out of 10, average loss across batches: 494.7087872444315\n",
      "Finished local epoch 8 out of 10, average loss across batches: 495.1939821933178\n",
      "Finished local epoch 9 out of 10, average loss across batches: 494.347697546127\n",
      "Finished local epoch 10 out of 10, average loss across batches: 494.4248227383228\n",
      "Training user 2 in round 10\n",
      "Finished local epoch 1 out of 10, average loss across batches: 502.4857522955847\n",
      "Finished local epoch 2 out of 10, average loss across batches: 501.16754274670353\n",
      "Finished local epoch 3 out of 10, average loss across batches: 500.36362840471224\n",
      "Finished local epoch 4 out of 10, average loss across batches: 499.5220081450173\n",
      "Finished local epoch 5 out of 10, average loss across batches: 499.04175729881047\n",
      "Finished local epoch 6 out of 10, average loss across batches: 499.02510050087494\n",
      "Finished local epoch 7 out of 10, average loss across batches: 498.6972141179564\n",
      "Finished local epoch 8 out of 10, average loss across batches: 498.3191938443421\n",
      "Finished local epoch 9 out of 10, average loss across batches: 497.56663451043727\n",
      "Finished local epoch 10 out of 10, average loss across batches: 497.2484060434195\n",
      "Training user 3 in round 10\n",
      "Finished local epoch 1 out of 10, average loss across batches: 490.8298591092887\n",
      "Finished local epoch 2 out of 10, average loss across batches: 490.2130095087381\n",
      "Finished local epoch 3 out of 10, average loss across batches: 490.2256066502338\n",
      "Finished local epoch 4 out of 10, average loss across batches: 489.3004502139417\n",
      "Finished local epoch 5 out of 10, average loss across batches: 488.63098708309803\n",
      "Finished local epoch 6 out of 10, average loss across batches: 488.7543136412839\n",
      "Finished local epoch 7 out of 10, average loss across batches: 488.1773377690449\n",
      "Finished local epoch 8 out of 10, average loss across batches: 487.671824627612\n",
      "Finished local epoch 9 out of 10, average loss across batches: 488.41412439308016\n",
      "Finished local epoch 10 out of 10, average loss across batches: 489.2078039942975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [16:04<00:00, 96.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS AT GLOBAL ROUND 10 totalL: 500.52906510772704\n",
      "TRAINING ALL DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:57:05.692256Z",
     "start_time": "2024-05-26T19:57:05.539689Z"
    }
   },
   "cell_type": "code",
   "source": "ClientDatasetManager.plot_dataset_splits(res.client_datasets)\n",
   "id": "ffd435fe6e7a32de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res.global_model = res.global_model.to('cuda')",
   "id": "b279b8e653cbea51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to('cuda')\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1])"
   ],
   "id": "6d37f3e0d12b9bf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_reconstructed(res.global_model, r0=(-3, 3), r1=(-3, 3))\n",
   "id": "9c71623690560cdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "258b275bbc07c1b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trial_weights = calculate_new_weights(res.global_model.encoder, res.client_datasets, 0.5,0.0)\n",
   "id": "43e0f1a902fff376",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Original Weights: {calculate_relative_dataset_sizes(res.client_datasets)}\")\n",
    "print(f\"New Weights: {trial_weights}\")"
   ],
   "id": "6eda7afb04ae1dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    ",
   "id": "d3b10d3ec44c840c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "second_res = federate(FakeArgs(), trial_weights, res.client_datasets)\n",
    "second_res.serialise(\"bogus_point_one\",FakeArgs())\n"
   ],
   "id": "4b95e47191222519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a = res.global_loss_manager.validation_total_across_communication",
   "id": "8db06cdf7ba87dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "b = second_res.global_loss_manager.validation_total_across_communication",
   "id": "8b25e1543e3f6757",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(zip(a,b))",
   "id": "a2668b9ad5d6ca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb032765b840b084",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
